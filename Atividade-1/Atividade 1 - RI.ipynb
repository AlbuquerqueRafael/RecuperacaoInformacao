{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "AND = 1\n",
    "OR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download o dado CSV\n",
    "# Dependendo de sua internet, este comando pode demorar\n",
    "def download_csv():\n",
    "    r = requests.get('https://drive.google.com/a/ccc.ufcg.edu.br/uc?authuser=0&id=1YqUakSw2BLj-Hgz_oPntvHWwK6vKxjOb&output=csv')\n",
    "    data = r.content\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Formata o CSV como um Data Frame \n",
    "# Contéudo do DF:\n",
    "# titulo: Titulo da Noticia\n",
    "# conteudo: Contéudo da Noticia\n",
    "# idNoticia: Id da noticia\n",
    "# content: O titulo + ' ' conteudo (Concatenção do titulo e do conteudo separado por um ' ') \n",
    "def getCSVFormattedAsDF():\n",
    "    data = download_csv()\n",
    "    df = pd.read_csv(io.BytesIO(data))\n",
    "    df[\"content\"] = df[\"titulo\"] + ' ' + df[\"conteudo\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cria e retorna um dicionario de tokens. Cada chave do dicionario possui como valor um conjunto(set) de noticias em que \n",
    "# este token aparece. \n",
    "def init_tokens():\n",
    "    tokens = {}\n",
    "    df = getCSVFormattedAsDF()\n",
    "    sizeDF = len(df[\"idNoticia\"])\n",
    "    \n",
    "    for idx in range(sizeDF):\n",
    "        content = df[\"content\"][idx]\n",
    "        words = nltk.word_tokenize(content)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                tokens[word].add(df[\"idNoticia\"][idx]) \n",
    "            except KeyError:\n",
    "                tokens[word] = set()\n",
    "                tokens[word].add(df[\"idNoticia\"][idx]) \n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna o conjunto de noticias onde o tokens aparece. Caso o tokens não exista nos dados, retorna um set vazio. \n",
    "def getTokenInfo(key, tokens):\n",
    "    if (key not in tokens):\n",
    "        return set()\n",
    "    else:\n",
    "        return tokens[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna o conjunto de tokens da operação pesquisa. \n",
    "def search(ipt, tokens):\n",
    "    if (ipt.strip() == \"\"):\n",
    "        return \"Please, inform at least one token\"\n",
    "    \n",
    "    ipt = ipt.split(\" \")\n",
    "    init = ipt[0].lower()\n",
    "    size = len(ipt)\n",
    "    \n",
    "    if (size == 1) :\n",
    "        if init not in tokens:\n",
    "            return \"This token is not present in any news\"\n",
    "        else:\n",
    "            return tokens[init]\n",
    "    else:\n",
    "        result = set(getTokenInfo(init,tokens))\n",
    "        command = \"\"\n",
    "        for index in range(1,size):\n",
    "            if (index % 2 == 0):\n",
    "                compare = getTokenInfo(ipt[index].lower(), tokens)\n",
    "    \n",
    "                if  command == AND:\n",
    "                    result = result & compare\n",
    "                else:\n",
    "                    result = result | compare              \n",
    "            else:\n",
    "                if (ipt[index] != \"AND\" and ipt[index] != \"OR\"):\n",
    "                    return \"Incorrect input. The tokens searched only can be separated by AND-OR\"\n",
    "                elif ipt[index] == \"AND\":\n",
    "                    command = AND\n",
    "                else:\n",
    "                    command = OR\n",
    "    \n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia o dicionario de tokens\n",
    "tokens = init_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(search(\"Campina AND Grande\", tokens)) == 12\n",
    "assert len(search(\"debate OR presidencial\", tokens)) == 1770\n",
    "assert len(search(\"debate AND presidencial\", tokens)) == 201\n",
    "assert len(search(\"presidenciáveis OR corruptos\", tokens)) == 164\n",
    "assert len(search(\"presidenciáveis AND corruptos\", tokens)) == 0\n",
    "assert len(search(\"Belo OR Horizonte\", tokens)) == 331\n",
    "assert len(search(\"Belo AND Horizonte\", tokens)) == 242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1036, 1068, 1370, 1770, 1952, 1987, 2763, 2777, 2779, 4802, 5382, 5870, 6694}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
