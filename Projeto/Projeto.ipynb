{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import simplejson as json\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "<div align=\"justify\">\n",
    "   <p> \t&nbsp;\t&nbsp;\t&nbsp;Esse documento tem como objetivo analisar classificação de notícias vs sátiras. O intuito por trás dessa analise é tentar verificar as diferenças entre estes dois tipos de informação. O objetivo por trás disso é tentar a partir de uma base claramente falsa(sátiras), verificar nuances em relação a uma base de notícia reais. Isto pode ser interessante, pois como sátiras e fake news são notícias falsas, pode ser possível que ao se treinar um modelo utilizando sátiras(que possui base de dados maior disponível), possa ser possível identificar fake news. Contudo, não analisaremos esta possivel correlação neste documento.</p>\n",
    "    <br>\n",
    "   \t&nbsp;\t&nbsp;\t&nbsp;A base de dados que iremos utilizar está disponível em: <a href=\"https://drive.google.com/file/d/0B7MeyftEk73Wbk1MQTJzeWVtRU1FaDBTb3dwTy1yVkc1MHVz/view?usp=sharing\"> base de dados</a>. Nela encontraremos 8748 notícias, sendo elas 4374 categorizadas do tipo 'REAL' e 4374 com a categorização 'FAKE'. As notícias reais foram retiradas de sites de notícias com fontes confiáveis, enquanto as notificas do tipo fake foram retiradas de sites de humor, principalmente do <a href=\"http://sensacionalista.com.br\"> sensacionalista </a>. Para a limpeza de dados, foi decidido utilizar funções para remover tudo que não era alfabético ou que fosse uma <a href=\"https://pt.wikipedia.org/wiki/Palavra_vazia\" > palavra vazia. </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções auxiliares:\n",
    "\n",
    "Para realizar esta análise foram criadas algumas funções auxiliares que estão descritas a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para remover as pontuações, caracteres não alfabeticos dos documentos e stopwords\n",
    "def removeNotRelevantWordsFromRow(row):\n",
    "    stopword = stopwords.words('portuguese')\n",
    "    stopword.append(\"é\")\n",
    "    words = nltk.word_tokenize(row)\n",
    "    words= [word.lower() for word in words if word.isalpha()]\n",
    "    wordsFormatted = [word.lower() for word in words if word not in stopword]\n",
    "\n",
    "    return ' '.join(wordsFormatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNotRelevantWordsFromDF(df):\n",
    "    auxDF = df\n",
    "    size = len(auxDF)\n",
    "    for index in range(size):\n",
    "        auxDF.text[index] = removeNotRelevantWordsFromRow(auxDF.text[index])\n",
    "    \n",
    "    return auxDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFrameFromCSV():\n",
    "    df = pd.read_csv('base_dados.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    lista = list(zip(*[input_list[i:] for i in range(n)]))\n",
    "    \n",
    "    if (len(lista) == 0):\n",
    "        return \"\"\n",
    "    \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLabelIDColuna(row):\n",
    "    if (row == 'FAKE'):\n",
    "        return 0\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideFakeAndReal(df):\n",
    "    return df[df.label == 'REAL'], df[df.label == 'FAKE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfIDFAndFeaturesFromDf(df):\n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=10, norm='l2', encoding='utf-8', ngram_range=(1, 2))\n",
    "    features = tfidf.fit_transform(df.text).toarray()\n",
    "    return tfidf, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsRankingFromTFIDF(tfidf, features):\n",
    "    terms = tfidf.get_feature_names()\n",
    "\n",
    "    # sum tfidf frequency of each term through documents\n",
    "    sums = features.sum(axis=0)\n",
    "    data = []\n",
    "\n",
    "    for col, term in enumerate(terms):\n",
    "        data.append( (term, sums[col] ))\n",
    "\n",
    "    ranking = pd.DataFrame(data, columns=['term','rank'])\n",
    "    ranking.sort_values('rank',inplace=True, ascending=False)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando as palavras e bigramas mais relevantes nos dataframes:\n",
    "    Nesta parte do projeto, iremos analisar as palavras e os bigramas mais importantes do conjunto de dados REAL e do conjunto de dados FAKE. Essa relevância será baseada e ranqueada com base no TFIDF.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getDataFrameFromCSV()\n",
    "realDF, fakeDF = divideFakeAndReal(df)\n",
    "#Fix Index\n",
    "realDF.index = range(4375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rafael Albuquerque\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Rafael Albuquerque\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "realDF = removeNotRelevantWordsFromDF(realDF)\n",
    "fakeDF = removeNotRelevantWordsFromDF(fakeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfREAL, featuresREAL = getTfIDFAndFeaturesFromDf(realDF)\n",
    "tfidfFAKE, featuresFAKE = getTfIDFAndFeaturesFromDf(fakeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir da função shape nas features, obtemos a seguinte tabela:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table> \n",
    "    <thead> \n",
    "        <th> # </th> \n",
    "        <th> Textos </th> \n",
    "        <th> Features</th> \n",
    "    </thead> \n",
    "    <tbody> \n",
    "        <tr>\n",
    "            <td> REAL </td> \n",
    "            <td> 4374 </td> \n",
    "            <td> 21726 </td> \n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td> FAKE </td> \n",
    "          <td> 4374 </td> \n",
    "          <td> 5432</td> \n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos então que nas 4374 linhas de texto de ambos os dataframes, enquanto o dataframe REAL tem 21726 features(conjuto de palavras e bigramas), o fake possui apenas 5432 para as mesmas 4374 linhas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankingFAKE = getWordsRankingFromTFIDF(tfidfFAKE, featuresFAKE)\n",
    "rankingREAL = getWordsRankingFromTFIDF(tfidfREAL, featuresREAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora aqui temos as 10 palavras e os bigramas mais relevantes(com base no TFIDF) do conjunto de dados fake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>disse</td>\n",
       "      <td>112.573469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680</th>\n",
       "      <td>ser</td>\n",
       "      <td>82.607144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>presidente</td>\n",
       "      <td>73.242460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>dilma</td>\n",
       "      <td>72.293398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>hoje</td>\n",
       "      <td>68.034295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>vai</td>\n",
       "      <td>64.176920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4612</th>\n",
       "      <td>segundo</td>\n",
       "      <td>63.594587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>brasil</td>\n",
       "      <td>62.723910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4484</th>\n",
       "      <td>rio</td>\n",
       "      <td>62.612771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4908</th>\n",
       "      <td>ter</td>\n",
       "      <td>61.731067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term        rank\n",
       "1591       disse  112.573469\n",
       "4680         ser   82.607144\n",
       "4015  presidente   73.242460\n",
       "1549       dilma   72.293398\n",
       "2477        hoje   68.034295\n",
       "5159         vai   64.176920\n",
       "4612     segundo   63.594587\n",
       "639       brasil   62.723910\n",
       "4484         rio   62.612771\n",
       "4908         ter   61.731067"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankingFAKE.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4486</th>\n",
       "      <td>[(rio, janeiro)]</td>\n",
       "      <td>43.161893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>[(dilma, rousseff)]</td>\n",
       "      <td>31.267293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>[(michel, temer)]</td>\n",
       "      <td>29.115406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>[(presidente, dilma)]</td>\n",
       "      <td>26.926771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>[(eduardo, cunha)]</td>\n",
       "      <td>23.926910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>[(redes, sociais)]</td>\n",
       "      <td>22.303762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>[(polícia, federal)]</td>\n",
       "      <td>19.971674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>[(aécio, neves)]</td>\n",
       "      <td>19.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>[(todo, mundo)]</td>\n",
       "      <td>19.298901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>[(pode, ser)]</td>\n",
       "      <td>18.573038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       term       rank\n",
       "4486       [(rio, janeiro)]  43.161893\n",
       "1553    [(dilma, rousseff)]  31.267293\n",
       "3136      [(michel, temer)]  29.115406\n",
       "4020  [(presidente, dilma)]  26.926771\n",
       "1745     [(eduardo, cunha)]  23.926910\n",
       "4358     [(redes, sociais)]  22.303762\n",
       "3881   [(polícia, federal)]  19.971674\n",
       "508        [(aécio, neves)]  19.358600\n",
       "4976        [(todo, mundo)]  19.298901\n",
       "3851          [(pode, ser)]  18.573038"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankingFAKE['term'] = rankingFAKE['term'].map(lambda text: find_ngrams(text.split(\" \"), 2))\n",
    "rankingFAKE[rankingFAKE.term != \"\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora aqui temos as 10 palavras mais relevantes(com base no TFIDF) do conjunto de dados real:\n",
    "ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15794</th>\n",
       "      <td>presidente</td>\n",
       "      <td>92.121205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>disse</td>\n",
       "      <td>83.909771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19847</th>\n",
       "      <td>temer</td>\n",
       "      <td>82.148183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443</th>\n",
       "      <td>federal</td>\n",
       "      <td>76.274089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18865</th>\n",
       "      <td>ser</td>\n",
       "      <td>73.927833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9410</th>\n",
       "      <td>governo</td>\n",
       "      <td>73.842090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19235</th>\n",
       "      <td>sobre</td>\n",
       "      <td>71.387218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>afirmou</td>\n",
       "      <td>69.924720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18557</th>\n",
       "      <td>segundo</td>\n",
       "      <td>68.317896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12557</th>\n",
       "      <td>ministro</td>\n",
       "      <td>65.393654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term       rank\n",
       "15794  presidente  92.121205\n",
       "6493        disse  83.909771\n",
       "19847       temer  82.148183\n",
       "8443      federal  76.274089\n",
       "18865         ser  73.927833\n",
       "9410      governo  73.842090\n",
       "19235       sobre  71.387218\n",
       "545       afirmou  69.924720\n",
       "18557     segundo  68.317896\n",
       "12557    ministro  65.393654"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankingREAL.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11252</th>\n",
       "      <td>[(lava, jato)]</td>\n",
       "      <td>64.591437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12435</th>\n",
       "      <td>[(michel, temer)]</td>\n",
       "      <td>48.486825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19665</th>\n",
       "      <td>[(supremo, tribunal)]</td>\n",
       "      <td>40.987895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12652</th>\n",
       "      <td>[(ministério, público)]</td>\n",
       "      <td>40.897214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>[(tribunal, federal)]</td>\n",
       "      <td>40.839011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6269</th>\n",
       "      <td>[(dilma, rousseff)]</td>\n",
       "      <td>38.248806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13785</th>\n",
       "      <td>[(operação, lava)]</td>\n",
       "      <td>34.691884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15840</th>\n",
       "      <td>[(presidente, michel)]</td>\n",
       "      <td>33.339170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7076</th>\n",
       "      <td>[(eduardo, cunha)]</td>\n",
       "      <td>25.615235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8520</th>\n",
       "      <td>[(federal, stf)]</td>\n",
       "      <td>25.553947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          term       rank\n",
       "11252           [(lava, jato)]  64.591437\n",
       "12435        [(michel, temer)]  48.486825\n",
       "19665    [(supremo, tribunal)]  40.987895\n",
       "12652  [(ministério, público)]  40.897214\n",
       "20558    [(tribunal, federal)]  40.839011\n",
       "6269       [(dilma, rousseff)]  38.248806\n",
       "13785       [(operação, lava)]  34.691884\n",
       "15840   [(presidente, michel)]  33.339170\n",
       "7076        [(eduardo, cunha)]  25.615235\n",
       "8520          [(federal, stf)]  25.553947"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankingREAL['term'] = rankingREAL['term'].map(lambda text: find_ngrams(text.split(\" \"), 2))\n",
    "rankingREAL[rankingREAL.term != \"\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que apesar de existir algumas features com iguais relevâncias nos dois conjuntos de dados, em geral as features de maior relevância diferem de um conjunto para o outro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando a classificação de notícias vs sátiras:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getDataFrameFromCSV()\n",
    "df['label_id'] = df['label'].map(lambda label: createLabelIDColuna(label))\n",
    "\n",
    "tfidf, features = getTfIDFAndFeaturesFromDf(df)\n",
    "labels = df['label_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta próxima etapa compareremos quatro métodos de classificação:\n",
    " - Regressão\n",
    " - Naive Bayes\n",
    " - Máquina de vetores de suporte (SVM)\n",
    " - Random Forest\n",
    "\n",
    "Para comparar, utilizaremos <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\"> validação cruzada </a> disponivel pelo sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "LinearSVC                 0.984909\n",
       "LogisticRegression        0.977823\n",
       "MultinomialNB             0.955881\n",
       "RandomForestClassifier    0.936563\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver, a partir dos dados acima, que a acurácia dos métodos se mostrou bastante parecida, com apenas uma leve vantagem para o SVM(Descrito como LinearSVC). Mostrando assim que há indícios que aspectos textuais de sátiras são bem diferentes de aspectos textuais de notícias verdadeiras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pequena aplicação:\n",
    "\n",
    "Para esta analise, foi criada uma pequena aplicação, onde o leitor pode colocar um texto e escolher o classificador. A partir disso será retornado uma predição que informará se a notícia é falsa ou verdadeira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label_id'], random_state = 0)\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_train)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    return X_train_tfidf, y_train, count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassifier(option, X_train_tfidf, y_train):\n",
    "    if (option == \"regress\"):\n",
    "        clf = LogisticRegression(random_state=0).fit(X_train_tfidf, y_train)\n",
    "    elif (option == \"naive\"):\n",
    "        clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "    elif (option == \"svm\"):\n",
    "        clf = LinearSVC().fit(X_train_tfidf, y_train)\n",
    "    elif (option == \"rf\"):\n",
    "        clf = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0).fit(X_train_tfidf, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkData(data, method):\n",
    "    df = getDataFrameFromCSV()\n",
    "    df['label_id'] = df['label'].map(lambda label: createLabelIDColuna(label))\n",
    "    X_train_tfidf, y_train, count_vect = trainModel(df)\n",
    "    clf = getClassifier(method, X_train_tfidf, y_train)\n",
    "    dataArray = []\n",
    "    dataArray.append(data)\n",
    "    result = clf.predict(count_vect.transform(dataArray))\n",
    "    \n",
    "    if (len(result) > 0):\n",
    "        if (result[0] == 0):\n",
    "            print ('FAKE')\n",
    "        elif (result[0] == 1):\n",
    "            print ('REAL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "<head>\n",
       "    <script>\n",
       "    function handle_output(out){\n",
       "      alert(out.content.text);\n",
       "\n",
       "    }\n",
       "\n",
       "    \n",
       "    function predict() {\n",
       "      var element = document.getElementById(\"options\");\n",
       "      var textArea = document.getElementById(\"textArea\");\n",
       "      var content = textArea.value;\n",
       "      var option = element.options[element.selectedIndex].value;\n",
       "      var callbacks = {iopub : { output : handle_output}}\n",
       "      var kernel = IPython.notebook.kernel;\n",
       "      var command = \"checkData('\"+content+ \"','\" + option + \"')\"\n",
       "      var msg_id = kernel.execute(command, callbacks);\n",
       "\n",
       "    }\n",
       "    </script>\n",
       "</head>\n",
       "<body>\n",
       "  <select id=\"options\">\n",
       "    <option value=\"regress\">Regressão</option>\n",
       "    <option value=\"naive\">Naive Bayes</option>\n",
       "    <option value=\"svm\">SVM</option>\n",
       "    <option value=\"rf\">Random Forest</option>\n",
       "  </select>\n",
       "  <button onclick=\"predict()\">Predict</button>\n",
       "  <div>\n",
       "    <textarea id=\"textArea\" rows=\"20\" cols=\"80\">\n",
       "    </textarea>\n",
       "  </div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <script>\n",
    "    function handle_output(out){\n",
    "      alert(out.content.text);\n",
    "\n",
    "    }\n",
    "\n",
    "    \n",
    "    function predict() {\n",
    "      var element = document.getElementById(\"options\");\n",
    "      var textArea = document.getElementById(\"textArea\");\n",
    "      var content = textArea.value;\n",
    "      var option = element.options[element.selectedIndex].value;\n",
    "      var callbacks = {iopub : { output : handle_output}}\n",
    "      var kernel = IPython.notebook.kernel;\n",
    "      var command = \"checkData('\"+content+ \"','\" + option + \"')\"\n",
    "      var msg_id = kernel.execute(command, callbacks);\n",
    "\n",
    "    }\n",
    "    </script>\n",
    "</head>\n",
    "<body>\n",
    "  <select id=\"options\">\n",
    "    <option value=\"regress\">Regressão</option>\n",
    "    <option value=\"naive\">Naive Bayes</option>\n",
    "    <option value=\"svm\">SVM</option>\n",
    "    <option value=\"rf\">Random Forest</option>\n",
    "  </select>\n",
    "  <button onclick=\"predict()\">Predict</button>\n",
    "  <div>\n",
    "    <textarea id=\"textArea\" rows=\"20\" cols=\"80\">\n",
    "    </textarea>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://drive.google.com/file/d/1qjzdlTX4jJp60jm3EnOBqmNflZFYhhSR/view?usp=sharing\"> <b>Video demonstrativo </b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão e trabalhos futuros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Conclui-se, então, que em todos os classsificadores utilizados, há sim diferença entre uma notícia do tipo sátira e uma do tipo real. Também podemos notar que há indicios que as palavras utilizadas neste tipo de notícia(sátira) são bem diferentes das utilizadas em notícias reais. \n",
    "   <br>\n",
    "   <br>\n",
    "    Para futuros estudos, pensa-se se em aumentar a base de dados e também incluir além do texto da notícia, o título. Também se pensa em analisar/classificar individualmente as palavras e os bigramas. Por último, pode vir a ser interessante analisar a correlação entre fake news e sátiras, pois, caso haja uma alta correlação, o modelo proposto pode vir a ser uma boa alternativa para se descobrir fake news."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
