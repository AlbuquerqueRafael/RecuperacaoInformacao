{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "AND = 1\n",
    "OR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download o dado CSV\n",
    "def download_csv():\n",
    "    r = requests.get('https://drive.google.com/a/ccc.ufcg.edu.br/uc?authuser=0&id=1YqUakSw2BLj-Hgz_oPntvHWwK6vKxjOb&output=csv')\n",
    "    data = r.content\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Formata o CSV como um Data Frame \n",
    "# Contéudo do DF:\n",
    "# titulo: Titulo da Noticia\n",
    "# conteudo: Contéudo da Noticia\n",
    "# idNoticia: Id da noticia\n",
    "# content: O titulo + ' ' conteudo (Concatenção do titulo e do conteudo separado por um ' ') \n",
    "def getCSVFormattedAsDF():\n",
    "    data = download_csv()\n",
    "    df = pd.read_csv(io.BytesIO(data))\n",
    "    df[\"content\"] = df[\"titulo\"] + ' ' + df[\"conteudo\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cria e retorna um dicionario de tokens. Cada chave do dicionario possui como valor um conjunto(set) de noticias em que \n",
    "# este token aparece. \n",
    "def init_tokens():\n",
    "    tokens = {}\n",
    "    df = getCSVFormattedAsDF()\n",
    "    sizeDF = len(df[\"idNoticia\"])\n",
    "    \n",
    "    for idx in range(sizeDF):\n",
    "        content = df[\"content\"][idx]\n",
    "        words = nltk.word_tokenize(content)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                tokens[word].add(df[\"idNoticia\"][idx]) \n",
    "            except KeyError:\n",
    "                tokens[word] = set()\n",
    "                tokens[word].add(df[\"idNoticia\"][idx]) \n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna o conjunto de tokens da operação pesquisa. \n",
    "def search(ipt, tokens):\n",
    "    if (ipt.strip() == \"\"):\n",
    "        return \"Please, inform at least one token\"\n",
    "    \n",
    "    ipt = ipt.split(\" \")\n",
    "    init = ipt[0].lower()\n",
    "    size = len(ipt)\n",
    "    \n",
    "    if (size == 1) :\n",
    "        if init not in tokens:\n",
    "            return \"This token is not present in any news\"\n",
    "        else:\n",
    "            return tokens[init]\n",
    "    else:\n",
    "        result = set(tokens[init])\n",
    "        comand = \"\"\n",
    "        for index in range(1,size):\n",
    "            if ipt[index] == \"AND\":\n",
    "                comand = AND\n",
    "            elif ipt[index] == \"OR\":\n",
    "                comand = OR\n",
    "            elif ipt[index].lower() in tokens:\n",
    "                if comand == AND:\n",
    "                    result = (tokens[ipt[index].lower()]  & result)\n",
    "                elif comand == OR:\n",
    "                    result |= (tokens[ipt[index].lower()] | result)\n",
    "                comand = \"\"\n",
    "            else:\n",
    "                comand = \"\"\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia o dicionario de tokens\n",
    "tokens = init_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(search(\"Campina AND Grande\", tokens)) == 12\n",
    "assert len(search(\"debate OR presidencial\", tokens)) == 1770\n",
    "assert len(search(\"debate AND presidencial\", tokens)) == 201\n",
    "assert len(search(\"presidenciáveis OR corruptos\", tokens)) == 164\n",
    "assert len(search(\"presidenciáveis AND corruptos\", tokens)) == 0\n",
    "assert len(search(\"Belo OR Horizonte\", tokens)) == 331\n",
    "assert len(search(\"Belo AND Horizonte\", tokens)) == 242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
